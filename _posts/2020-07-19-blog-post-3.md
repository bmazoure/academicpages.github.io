---
title: 'Deep Reinforcement and InfoMax Learning'
date: 2020-07-19
permalink: /posts/deep-rl-infomax-learning/
description: Overview of our recent work on incorporating model transition dynamics into convolutions using the infoNCE loss.
tags:
  - Python
  - PyTorch
  - Introduction
  - Machine learning
  - InfoMax
  - Deep InfoMax
  - Noise contrastive estimation
  - Self-supervised learning
  - Reinforcement learning
  - Procgen
---

Recent advances in the area of self-supervised learning on pixel data (e.g. DIM, ST-DIM, CPC, MoCo, SIMPLE, BYOL) motivate the application of similar techniques in reinforcement learning.

A number of recent papers (e.g. CURL, DrQ) suggest that data augmentation, coupled with the exponential moving average of the target encoder, is a good-enough way to improve the model's performance on standard benchmarks in discrete and continuous control.

Our recent joint work with the Microsoft Research Montreal lab rather looks into the predictive capabilities of classic value-based models and how they can be enhanced. Specifically, suppose we are given an RL agent, which is placed in a given state $s_t$ at time $t$. Furthermore, suppose that this agent currently follows the policy $\pi$ (this can be relaxed further down). We are interested in thow the current state $s_t$ and current action $a_t$ are able to predict the next state $s_{t+1}$ under $\pi$. Even though $\pi$ might not be optimal, predicting the next state helps the model keep a relevant set of features through the entire task (i.g. some form of PSR).

The quantity which allows us to boost this state-action-next-state similarity is the mutual information between the coupling $(S_t,A_t)$ and $S_{t+1}$:

$$
\mathcal{I}[S_t,A_t||S_{t+1}]=\int_{s_{t+1},s_t,a_t} \log \frac{p(s_{t+1}|s_t,a_t)}{p(s_{t+1})}dP(s_{t+1},s_t,a_t)
$$

We noted that simply looking at the information between $S_t$ and $S_{t+1}$ was not sufficient to reliably estimate the next state $S_{t+1}$, since the policy was averaged out in the expression.

![png](/files/driml/fig7_model-01.png)

------