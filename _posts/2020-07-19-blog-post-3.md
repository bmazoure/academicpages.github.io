---
title: 'Deep Reinforcement and InfoMax Learning'
date: 2020-07-19
permalink: /posts/deep-rl-infomax-learning/
description: Overview of our recent work on incorporating model transition dynamics into convolutions using the infoNCE loss.
tags:
  - Python
  - PyTorch
  - Introduction
  - Machine learning
  - InfoMax
  - Deep InfoMax
  - Noise contrastive estimation
  - Self-supervised learning
  - Reinforcement learning
  - Procgen
---

Recent advances in the area of self-supervised learning on pixel data (e.g. DIM, ST-DIM, CPC, MoCo, SIMPLE, BYOL) motivate the application of similar techniques in reinforcement learning.

A number of recent papers (e.g. CURL, DrQ) suggest that data augmentation, coupled with the exponential moving average of the target encoder, is a good-enough way to improve the model's performance on standard benchmarks in discrete and continuous control.

Our recent joint work with the Microsoft Research Montreal lab (link to paper: [link](https://arxiv.org/abs/2006.07217)) rather looks into the predictive capabilities of classic value-based models and how they can be enhanced. Specifically, suppose we are given an RL agent, which is placed in a given state $s_t$ at time $t$. Furthermore, suppose that this agent currently follows the policy $\pi$ (this can be relaxed further down). We are interested in thow the current state $s_t$ and current action $a_t$ are able to predict the next state $s_{t+1}$ under $\pi$. Even though $\pi$ might not be optimal, predicting the next state helps the model keep a relevant set of features through the entire task (i.g. some form of PSR).

The quantity which allows us to boost this state-action-next-state similarity is the mutual information between the coupling $(S_t,A_t)$ and $S_{t+1}$:

$$
\mathcal{I}[S_t,A_t\mid\mid S_{t+1}]=\int_{s_{t+1},s_t,a_t} \log \frac{p(s_{t+1}\mid s_t,a_t)}{p(s_{t+1})}dP(s_{t+1},s_t,a_t)
$$

We noted that simply looking at the information between $S_t$ and $S_{t+1}$ was not sufficient to reliably estimate the next state $S_{t+1}$, since the policy was averaged out in the expression.

Of course, the above formulation only captures one-step transition dynamics, so we can further generalize it by trying to predict the $k^{th}$ state from the current state-action pair:

$$
\mathcal{I}[S_t,A_t\mid\mid S_{t+k}]=\int_{s_{t+k},s_t,a_t} \log \frac{p(s_{t+k}\mid s_t,a_t)}{p(s_{t+k})}dP(s_{t+k},s_t,a_t),\;k>0
$$

The above quantity can be approximately measured by maximizing the infoNCE bound (van Oord, 2018), which requires to specify a distribution over positive and negative samples. In our case, to conserve the data efficency of off-policy algorithms, both distributions come from a randomly sampled batch of $(S_t,A_t,R_{t+1},S_{t+1},...,S_{t+k})$ tuples from the replay buffer. In fact, by performing a clever outer product along the batch dimension (pytorch code in paper), we are able to obtain an $n_{batch} \times n_{batch}$ similarity matrix, where the entry at position $(i,j)$ is equivalent to $\mathcal{I}[S_i,A_i\mid\mid S_j]$. The diagonal contains positive samples ($n_{batch}$ of them), while all off-diagonal entries ($n_{batch}(n_{batch}-1)$ of them) are taken to be negative samples.

The final model is inspired from AM-DIM, which maximizes the mutual information between different layers of the encoder (here, between conv layers and fc layers); it is schematically shown below.


![png](/files/driml/fig7-model-01.png)


#### PacMan results

Since the auxiliary loss function does not have an explicit reward-boosting term, it is not expected to perform extremelly well on reward maximization tasks. However, the convolutions and fully-connected layers now have a notion of dynamics (i.e. how entities change with time) for environments on which samples we train the model on. Which is precisely why we tested the performance of a C51 agent augmented with our loss in a simple PacMan-style continual setting (see **a**). The agent is sequentially given access to 4 tasks, in each of which one of the four ghosts is deadly, but all four ghosts have identical transition dynamics. 

It can be seen from the figure **c** below how the training rewards progress, as a function of training episodes, for the vanilla C51 agent, as well as for the C51 agent augmented with the infomax loss. Our loss forces the agent to keep track of all features predictive of next state, which is why it helps mitigate catastrophic forgetting in circumstances where the reward function can spontaneously change.

![png](/files/driml/fig3-pocman-nce-eps-01.png)

#### Procgen results

We also examined the performance of our model on the Procgen framework, where we let all agents train for 50M timesteps on 500 fixed levels, using only the DQN Nature encoder architecture (hence the reduced training setup). Training-time results are reported below.

![png](/files/driml/procgen_table.PNG)



------